# -*- coding: utf-8 -*-
"""Data_Exploration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17KloX51Tvm2V32jbfutgeseSjfXgeSH9
"""

import os
import numpy as np
import pandas as pd
import seaborn as sb
import matplotlib.pyplot as plt
import re
from textblob import TextBlob
from sklearn.model_selection import train_test_split
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

from google.colab import drive
drive.mount('/content/drive')

# upload dataset
file_path='/content/Combined_News_DJIA.csv'
file_path2='/content/upload_DJIA_table.csv'

df=pd.read_csv(file_path)
df2=pd.read_csv(file_path2)

df.head()

merge=df.merge(df2,how="inner",on="Date")

merge.head()

headline=[]
for row in range(0,len(merge.index)):
    headline.append(" ".join(str(x) for x in merge.iloc[row,2:27]))

headline

import re
clean_headline=[]
for i in range(0,len(headline)):
    clean_headline.append(re.sub("b[(')]",'',headline[i])) #remove b'
    clean_headline[i]=re.sub('b[(")]','',clean_headline[i]) #remove b"
    clean_headline[i]=re.sub("\'",'',clean_headline[i]) #remove \'

import re
clean_headline=[]
for i in range(0,len(headline)):
    clean_headline.append(re.sub("b[(')]",'',headline[i])) #remove b'
    clean_headline[i]=re.sub('b[(")]','',clean_headline[i]) #remove b"
    clean_headline[i]=re.sub("\'",'',clean_headline[i]) #remove \'

clean_headline

merge['Combined_news']=clean_headline
merge

descriptive_stats = merge.describe()

numerical_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']
merge[numerical_features].hist(bins=15, figsize=(15, 10))
plt.figure(figsize=(10, 8))
correlation_matrix = merge[numerical_features].corr()
sb.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()
print(descriptive_stats)

from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string

import nltk
nltk.download('punkt')
nltk.download('stopwords')

all_headlines = ' '.join(merge['Combined_news'])


tokens = word_tokenize(all_headlines)

# Convert to lowercase and remove punctuation and stopwords
tokens = [w.lower() for w in tokens if w.isalpha()]
stop_words = set(stopwords.words('english'))
tokens = [w for w in tokens if not w in stop_words]


word_freq = Counter(tokens)
common_words = word_freq.most_common(10)
merge['Word_Count'] = merge['Combined_news'].apply(lambda x: len(word_tokenize(x)))
average_word_count = merge['Word_Count'].mean()

# Visualize the distribituon
plt.figure(figsize=(10, 5))
sb.histplot(merge['Word_Count'], bins=30)
plt.title('Distribution of Headline Word Counts')
plt.xlabel('Word Count')
plt.ylabel('Frequency')
plt.show()


words, counts = zip(*common_words)
plt.figure(figsize=(10, 5))
plt.bar(words, counts)
plt.title('Most Common Words in Headlines')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.show()


print(common_words)


print(f'Average word count per headline: {average_word_count}')