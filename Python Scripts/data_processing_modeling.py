# -*- coding: utf-8 -*-
"""Data_Processing_Modeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nt-RdwAUqkoyWkPKsc1oOzkNG2RE9Hbl
"""

import os
import warnings
import re
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from textblob import TextBlob
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from xgboost import XGBClassifier
# from catboost import CatBoostClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

warnings.filterwarnings('ignore')

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

from google.colab import drive
drive.mount('/content/drive')

# Define the path to the directory
data_folder_path = '/content/drive/MyDrive/TUM/Advanced Seminar: Data Science in Finance/Data Science in Finance: Documents/Data'

file_path1 = os.path.join(data_folder_path, 'Combined_News_DJIA.csv')
file_path2 = os.path.join(data_folder_path, 'RedditNews.csv')
file_path3 = os.path.join(data_folder_path, 'upload_DJIA_table.csv')

combined_data = pd.read_csv(file_path1)
DJIA_data = pd.read_csv(file_path3)

file_path='/Combined_News_DJIA.csv'
file_path2='/upload_DJIA_table.csv'

combined_data=pd.read_csv(file_path)
DJIA_data=pd.read_csv(file_path2)

combined_data.head()

DJIA_data.head()

# Combining the data set with the Market price movement
full_data = combined_data.merge(DJIA_data, how="inner", on="Date")

full_data.head()

"""#Data Preparation"""

combined_data.isnull().sum()

# make a working copy
df1 = combined_data.copy()

# remove null values
df1.dropna(inplace=True)

# filling the null values with median

# df['Top23'].fillna(df['Top23'].median, inplace=True)
# df['Top24'].fillna(df['Top24'].median, inplace=True)
# df['Top25'].fillna(df['Top25'].median, inplace=True)

# function for cleaning the data
def clean_data(dataset):
    data = dataset.iloc[:,2:27]
    data.replace("[^a-zA-Z]", " ", regex=True, inplace=True)
    return data

# function for combining the headlines of all the columns into single column
def combine_data(data):
    headlines = []
    for i in range(len(data.index)):
        headlines.append(' '.join(str(x) for x in data.iloc[i, :]))
    return headlines

# function to perform lemmatization of the word
def lemmatize_data(data, lemmatizer):
    cleaned_dataset = []
    for i in range(len(data)):
        clean_text = data[i].lower()
        clean_text = clean_text.split()
        clean_text = [lemmatizer.lemmatize(word) for word in clean_text if word not in stopwords.words('english')]
        cleaned_dataset.append(' '.join(clean_text))
    return cleaned_dataset

# function to vectorize the data
def vectorize_data(data, cv):
    vectorized_dataset = cv.fit_transform(data)
    return vectorized_dataset

# function to find the plority of a sentence
def analize_sentiment(text):
    analysis = TextBlob((str(text)))
    return analysis.polarity

# split with a chronological cut

train = df1[df1['Date'] < '2015-01-01']
test = df1[df1['Date'] > '2014-12-31']

print(train.shape, test.shape)

# clean train and test data
clean_train_data = clean_data(train)
clean_test_data = clean_data(test)

# combine the headlines in single column
comb_train_data = combine_data(clean_train_data)
comb_test_data = combine_data(clean_test_data)

lemmatizer = WordNetLemmatizer()

# lemmatize data
train_data = lemmatize_data(comb_train_data, lemmatizer)
test_data = lemmatize_data(comb_test_data, lemmatizer)

cv = CountVectorizer(ngram_range=(2,2))

# vectorize data
vec_train_data = vectorize_data(train_data, cv)
vec_test_data = cv.transform(test_data)

"""#Basic Modeling

Random forest without hyperparameter tuning
"""

# create classifier
rf_classifier = RandomForestClassifier(n_estimators=200, criterion='entropy')
rf_classifier.fit(vec_train_data, train['Label'])

# predict
predictions = rf_classifier.predict(vec_test_data)

cm = confusion_matrix(test['Label'], predictions)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# Display classification report
print("Classification Report:")
print(classification_report(test['Label'], predictions))

# accuracy and f1 scores

print('Accuracy:', accuracy_score(test['Label'], predictions))
print('F1 Score:', f1_score(test['Label'], predictions))

"""XGBoost without hyperparameter tuning"""

ngram = [1, 2, 3, 4, 5]

for i in ngram:
  cv = CountVectorizer(ngram_range=(i, i))

  # vectorize data
  vec_train_data = vectorize_data(train_data, cv)
  vec_test_data = cv.transform(test_data)

  xgb = XGBClassifier(random_state=1)
  xgb.fit(vec_train_data, train['Label'])
  predictions = xgb.predict(vec_test_data)

  accuracy = accuracy_score(test['Label'], predictions)
  f1 = f1_score(test['Label'], predictions)

  print('max number of features used : {}'.format(i))
  print('ngram_range ({}, {})'.format(i, i))
  print('Accuracy:', accuracy)
  print('F1 Score:', f1)

  matrix = confusion_matrix(test['Label'], predictions)
  print('confusion matrix : {}'.format(matrix))
  print('========================================')

# best n-gram configuration model

cv = CountVectorizer(ngram_range=(3, 3))

# vectorize data
vec_train_data = vectorize_data(train_data, cv)
vec_test_data = cv.transform(test_data)

xgb = XGBClassifier(random_state=1)
xgb.fit(vec_train_data, train['Label'])
predictions = xgb.predict(vec_test_data)

cm = confusion_matrix(test['Label'], predictions)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# Display classification report
print("Classification Report:")
print(classification_report(test['Label'], predictions))

# accuracy and f1 scores

print('Accuracy:', accuracy_score(test['Label'], predictions))
print('F1 Score:', f1_score(test['Label'], predictions))

"""Sentiment Analyisis"""

train_sentiment = train.drop(['Date', 'Label'], axis=1)

for column in train_sentiment:
    train_sentiment[column] = train_sentiment[column].apply(analize_sentiment)  #converting the train headlines into polarity scores
train_sentiment = train_sentiment+10  # removing negative coefficient from the datset for better performance

test_sentiment = test.drop(['Date', 'Label'], axis=1)

for column in test_sentiment:
    test_sentiment[column]=test_sentiment[column].apply(analize_sentiment) # converting the test headlines into ploarity
test_sentiment = test_sentiment+10 # removing negative coefficient from the datset for better performance

train_sentiment

# sentiment XGBoost model

xgb = XGBClassifier(random_state=1)
xgb.fit(train_sentiment, train['Label'])
predictions = xgb.predict(test_sentiment)

cm = confusion_matrix(test['Label'], predictions)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# Display classification report
print("Classification Report:")
print(classification_report(test['Label'], predictions))

# accuracy and f1 scores

print('Accuracy:', accuracy_score(test['Label'], predictions))
print('F1 Score:', f1_score(test['Label'], predictions))

"""#Data Preparation"""

# make a working copy
df2 = full_data.copy()

headlines = []
for row in range(0, len(df2.index)):
    headlines.append(" ".join(str(x) for x in df2.iloc[row,2:27]))

clean_headlines = []
for i in range(0, len(headlines)):
    clean_headlines.append(re.sub("b[(')]", '', headlines[i])) #remove b'
    clean_headlines[i]=re.sub('b[(")]', '', clean_headlines[i]) #remove b"
    clean_headlines[i]=re.sub("\'", '', clean_headlines[i]) #remove \'

df2['Combined_News'] = clean_headlines

"""#Advanced Modeling"""

import tqdm
import tensorflow as tf
from tensorflow import keras
from keras.callbacks import ModelCheckpoint
from keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split, cross_val_score
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
from bs4 import BeautifulSoup
import unicodedata
from nltk.tokenize import word_tokenize

# one hot encoding using keras tokenizer and pad sequencing
X = df2['Combined_News']
y = df2['Label']

# encoder = LabelEncoder()
# y = encoder.fit_transform(merge['Combined_News'])

print("shape of input data: ", X.shape)
print("shape of target variable: ", y.shape)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)

def remove_accented_characters(text):
    text =  unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
    return text

def stop_words(text):
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]
    filtered_sentence = []

    for w in word_tokens:
        if w not in stop_words:
            filtered_sentence.append(w)
    return filtered_sentence

def preprocessor_engine(text):
    corpus =[]
    for sent in tqdm.tqdm(text):
        sent = remove_accented_characters(sent)
        sent = stop_words(sent)
        corpus.append(sent)
    return corpus

train_data_pro = preprocessor_engine(X_train)

test_data_pro = preprocessor_engine(X_test)

tokenizer = Tokenizer(oov_token='<UNK>')
tokenizer.fit_on_texts(train_data_pro) # build the word index

train_sequences = tokenizer.texts_to_sequences(train_data_pro)
test_sequences = tokenizer.texts_to_sequences(test_data_pro)

MAX_SEQUENCE_LENGTH = 424

train_pad_sequences = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen = MAX_SEQUENCE_LENGTH, padding='post')
test_pad_sequneces = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen = MAX_SEQUENCE_LENGTH, padding='post')

total_words = len(tokenizer.word_index) + 1

from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense, Dropout
from keras.layers import Conv1D, SimpleRNN, Bidirectional, MaxPooling1D, GlobalMaxPool1D, LSTM, GRU
from keras.regularizers import L1L2
from keras.callbacks import EarlyStopping

model = Sequential()
model.add(Embedding(total_words, 300, input_length=MAX_SEQUENCE_LENGTH ))
model.add(Bidirectional(SimpleRNN(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))
model.add(Bidirectional(SimpleRNN(128, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)))
model.add(Bidirectional(SimpleRNN(64, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)))
model.add(Bidirectional(SimpleRNN(32, dropout=0.2, recurrent_dropout=0.2)))
model.add(Dropout(0.3))
model.add(Dense(1, activation='sigmoid'))
model.summary()

model.compile(optimizer="adam", loss='binary_crossentropy', metrics=['accuracy'])

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Train the model
history = model.fit(
    train_pad_sequences,
    y_train,
    batch_size=128,
    epochs=5,
    validation_split=0.1,
    callbacks=[early_stopping]
)

# Evaluate the model on the test dataset
test_loss, test_accuracy = model.evaluate(test_pad_sequneces, y_test, verbose=0)
print(f"Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}")

"""Linear Discriminant Analysis"""

!pip3 install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

def get_subjectivity(text):
    return TextBlob(text).sentiment.subjectivity

def get_polarity(text):
    return TextBlob(text).sentiment.polarity

def getSIA(text):
    sia = SentimentIntensityAnalyzer()
    sentiment = sia.polarity_scores(text)
    return sentiment

df2.head()

compound = []
neg = []
pos = []
neu = []
SIA = 0

for i in tqdm.tqdm(range(0, len(df2['Combined_News']))):
    SIA = getSIA(df2['Combined_News'][i])
    compound.append(SIA['compound'])
    neg.append(SIA['neg'])
    pos.append(SIA['pos'])
    neu.append(SIA['neu'])

df2['Subjectivity'] = df2['Combined_News'].apply(get_subjectivity)
df2['Polarity'] = df2['Combined_News'].apply(get_polarity)
df2['compound'] = compound
df2['neg'] = neg
df2['pos'] = pos
df2['neu'] = neu

df2['Subjectivity']

df_final = df2[['Label','Open', 'High', 'Low', 'Volume', 'Subjectivity', 'Polarity', 'compound', 'neg', 'pos', 'neu']]
df_quant = df2[['Label','Open', 'High', 'Low', 'Volume']]
df_final.head()

"""Modeling with the full data, news sentiment analysis included"""

X = df_final
X = df_final.drop(['Label'], axis=1).values

y = np.array(df_final['Label'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 0)

lda_model = LinearDiscriminantAnalysis().fit(X_train, y_train)

predictions = lda_model.predict(X_test)

import statsmodels.api as sm

# Assuming df is your DataFrame and it's already loaded with the data from the image
X1 = df2[['Open', 'High', 'Low', 'Volume', 'Subjectivity','Polarity','compound']]
X1 = sm.add_constant(X1)
y = df2['Label']
logit_model = sm.Logit(y, X1)
result = logit_model.fit()
summary = result.summary()
print(summary)

cm = confusion_matrix(y_test, predictions)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# Display classification report
print("Classification Report:")
print(classification_report(y_test, predictions))

# accuracy and f1 scores

print('Accuracy:', accuracy_score(y_test, predictions))
print('F1 Score:', f1_score(y_test, predictions))

"""Modeling with only quantitative data"""

X = df_quant
X = df_quant.drop(['Label'], axis=1).values

y = np.array(df_quant['Label'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 0)

lda_model = LinearDiscriminantAnalysis().fit(X_train, y_train)

predictions = lda_model.predict(X_test)

cm = confusion_matrix(y_test, predictions)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# Display classification report
print("Classification Report:")
print(classification_report(y_test, predictions))

# accuracy and f1 scores

print('Accuracy:', accuracy_score(y_test, predictions))
print('F1 Score:', f1_score(y_test, predictions))

"""Interesting Findings?! We can't reject the null hypothesis."""